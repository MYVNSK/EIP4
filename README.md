# EIP4
EIP online (Wednesday Batch)

Contributor: Mamidi Y V N Sandeep Kumar 

Score: [0.04119752720832262, 0.9911]

Definitions:

1. Convolutions: Convolution is extracting information from images
2. Filters/Kernels: filters extract the features of image.
3. Epoch: one iteration of training over entire data set
4. 1x1 convolution: 1x1 convolutions passes the input pixel entirely to the output. It doesn't consider the influence of adjacent pixels.
5. 3x3 convolution: 3x3 convolution uses a 3x3 matrix to process the input pixels to the output. 
6. Feature Maps: Feature Maps are output from convoultion. they are generated by the fitlers/kernel. 
7. Actication Function: Based on activation functions's result, the input is converted/passed to the output. 
8. Receptive Field: the set of pixesls being operated by convolutions filter/kernel.

Assignment2:
Total params: 12,796
Trainable params: 12,600
Non-trainable params: 196


Epoch 1/20

Epoch 00001: LearningRateScheduler setting learning rate to 0.003.
60000/60000 [==============================] - 15s 242us/step - loss: 0.5613 - acc: 0.8407 - val_loss: 0.1001 - val_acc: 0.9793
Epoch 2/20

Epoch 00002: LearningRateScheduler setting learning rate to 0.0022744503.
60000/60000 [==============================] - 11s 182us/step - loss: 0.2660 - acc: 0.9200 - val_loss: 0.0705 - val_acc: 0.9826
Epoch 3/20

Epoch 00003: LearningRateScheduler setting learning rate to 0.0018315018.
60000/60000 [==============================] - 11s 183us/step - loss: 0.2071 - acc: 0.9384 - val_loss: 0.0506 - val_acc: 0.9887
Epoch 4/20

Epoch 00004: LearningRateScheduler setting learning rate to 0.0015329586.
60000/60000 [==============================] - 11s 183us/step - loss: 0.1784 - acc: 0.9447 - val_loss: 0.0426 - val_acc: 0.9880
Epoch 5/20

Epoch 00005: LearningRateScheduler setting learning rate to 0.0013181019.
60000/60000 [==============================] - 11s 182us/step - loss: 0.1599 - acc: 0.9475 - val_loss: 0.0325 - val_acc: 0.9912
Epoch 6/20

Epoch 00006: LearningRateScheduler setting learning rate to 0.0011560694.
60000/60000 [==============================] - 11s 183us/step - loss: 0.1461 - acc: 0.9494 - val_loss: 0.0313 - val_acc: 0.9901
Epoch 7/20

Epoch 00007: LearningRateScheduler setting learning rate to 0.0010295127.
60000/60000 [==============================] - 11s 182us/step - loss: 0.1365 - acc: 0.9506 - val_loss: 0.0334 - val_acc: 0.9906
Epoch 8/20

Epoch 00008: LearningRateScheduler setting learning rate to 0.0009279307.
60000/60000 [==============================] - 11s 182us/step - loss: 0.1273 - acc: 0.9523 - val_loss: 0.0247 - val_acc: 0.9929
Epoch 9/20

Epoch 00009: LearningRateScheduler setting learning rate to 0.0008445946.
60000/60000 [==============================] - 11s 182us/step - loss: 0.1225 - acc: 0.9538 - val_loss: 0.0258 - val_acc: 0.9919
Epoch 10/20

Epoch 00010: LearningRateScheduler setting learning rate to 0.0007749935.
60000/60000 [==============================] - 11s 183us/step - loss: 0.1158 - acc: 0.9546 - val_loss: 0.0263 - val_acc: 0.9924
Epoch 11/20

Epoch 00011: LearningRateScheduler setting learning rate to 0.0007159905.
60000/60000 [==============================] - 11s 182us/step - loss: 0.1141 - acc: 0.9541 - val_loss: 0.0209 - val_acc: 0.9935
Epoch 12/20

Epoch 00012: LearningRateScheduler setting learning rate to 0.000665336.
60000/60000 [==============================] - 11s 183us/step - loss: 0.1118 - acc: 0.9548 - val_loss: 0.0220 - val_acc: 0.9934
Epoch 13/20

Epoch 00013: LearningRateScheduler setting learning rate to 0.0006213753.
60000/60000 [==============================] - 11s 184us/step - loss: 0.1077 - acc: 0.9548 - val_loss: 0.0209 - val_acc: 0.9937
Epoch 14/20

Epoch 00014: LearningRateScheduler setting learning rate to 0.0005828638.
60000/60000 [==============================] - 11s 184us/step - loss: 0.1062 - acc: 0.9557 - val_loss: 0.0197 - val_acc: 0.9936
Epoch 15/20

Epoch 00015: LearningRateScheduler setting learning rate to 0.0005488474.
60000/60000 [==============================] - 11s 182us/step - loss: 0.1027 - acc: 0.9553 - val_loss: 0.0193 - val_acc: 0.9934
Epoch 16/20

Epoch 00016: LearningRateScheduler setting learning rate to 0.0005185825.
60000/60000 [==============================] - 11s 182us/step - loss: 0.1017 - acc: 0.9557 - val_loss: 0.0200 - val_acc: 0.9939
Epoch 17/20

Epoch 00017: LearningRateScheduler setting learning rate to 0.000491481.
60000/60000 [==============================] - 11s 183us/step - loss: 0.1000 - acc: 0.9570 - val_loss: 0.0195 - val_acc: 0.9937
Epoch 18/20

Epoch 00018: LearningRateScheduler setting learning rate to 0.0004670715.
60000/60000 [==============================] - 11s 188us/step - loss: 0.0998 - acc: 0.9559 - val_loss: 0.0197 - val_acc: 0.9937
Epoch 19/20

Epoch 00019: LearningRateScheduler setting learning rate to 0.0004449718.
60000/60000 [==============================] - 11s 189us/step - loss: 0.0997 - acc: 0.9559 - val_loss: 0.0181 - val_acc: 0.9942
Epoch 20/20

Epoch 00020: LearningRateScheduler setting learning rate to 0.000424869.
60000/60000 [==============================] - 11s 187us/step - loss: 0.0961 - acc: 0.9566 - val_loss: 0.0186 - val_acc: 0.9941

Test Score
[0.018612720821413676, 0.9941]

Assignment3:

1. Base Model : Validation Accuracy: 83.13%
2. Modified Model: Validation Accuracy: 83.58%  (40th Epoch)
Model:
model = Sequential()
model.add(SeparableConv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3))) 
# Output: 32 * 32 * 32 and RF: 3
model.add(BatchNormalization())
model.add(SeparableConv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
# Output: 32*32*64 and RF: 5
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
# Output: 16*16*64 and RF: 6
model.add(Dropout(0.25))
model.add(SeparableConv2D(96, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
# Output: 16*16*96 and RF: 10
model.add(BatchNormalization())
model.add(SeparableConv2D(96, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
# Ouptut: 16*16*96 and RF:14
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
# Ouput: 8*8*96 and RF: 16
model.add(Dropout(0.25))
model.add(SeparableConv2D(108, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
# Output: 8*8*108 and RF: 24
model.add(BatchNormalization())
model.add(SeparableConv2D(108, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
# Output: 8*8*108 and RF: 32
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
# Output: 4*4*108 and RF: 36
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(30, activation='relu', kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Dropout(0.25))
model.add(Dense(num_classes, activation='softmax'))
# compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])



Epoch 1/50
390/390 [==============================] - 18s 47ms/step - loss: 1.6627 - acc: 0.3991 - val_loss: 1.4016 - val_acc: 0.4962
Epoch 2/50
390/390 [==============================] - 11s 27ms/step - loss: 1.2609 - acc: 0.5491 - val_loss: 1.0703 - val_acc: 0.6186
Epoch 3/50
390/390 [==============================] - 11s 28ms/step - loss: 1.0918 - acc: 0.6159 - val_loss: 0.9337 - val_acc: 0.6677
Epoch 4/50
390/390 [==============================] - 11s 28ms/step - loss: 0.9822 - acc: 0.6575 - val_loss: 0.8344 - val_acc: 0.7091
Epoch 5/50
390/390 [==============================] - 11s 29ms/step - loss: 0.9088 - acc: 0.6853 - val_loss: 0.7726 - val_acc: 0.7254
Epoch 6/50
390/390 [==============================] - 11s 28ms/step - loss: 0.8469 - acc: 0.7066 - val_loss: 0.7929 - val_acc: 0.7285
Epoch 7/50
390/390 [==============================] - 11s 28ms/step - loss: 0.7979 - acc: 0.7211 - val_loss: 0.7178 - val_acc: 0.7505
Epoch 8/50
390/390 [==============================] - 11s 29ms/step - loss: 0.7598 - acc: 0.7370 - val_loss: 0.7084 - val_acc: 0.7518
Epoch 9/50
390/390 [==============================] - 11s 29ms/step - loss: 0.7317 - acc: 0.7464 - val_loss: 0.7322 - val_acc: 0.7486
Epoch 10/50
390/390 [==============================] - 11s 28ms/step - loss: 0.6998 - acc: 0.7575 - val_loss: 0.6387 - val_acc: 0.7778
Epoch 11/50
390/390 [==============================] - 11s 28ms/step - loss: 0.6780 - acc: 0.7654 - val_loss: 0.6504 - val_acc: 0.7716
Epoch 12/50
390/390 [==============================] - 11s 29ms/step - loss: 0.6538 - acc: 0.7747 - val_loss: 0.6662 - val_acc: 0.7727
Epoch 13/50
390/390 [==============================] - 11s 28ms/step - loss: 0.6317 - acc: 0.7812 - val_loss: 0.6156 - val_acc: 0.7898
Epoch 14/50
390/390 [==============================] - 11s 29ms/step - loss: 0.6202 - acc: 0.7862 - val_loss: 0.6135 - val_acc: 0.7873
Epoch 15/50
390/390 [==============================] - 11s 29ms/step - loss: 0.6047 - acc: 0.7907 - val_loss: 0.6076 - val_acc: 0.7945
Epoch 16/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5887 - acc: 0.7984 - val_loss: 0.6241 - val_acc: 0.7911
Epoch 17/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5779 - acc: 0.8022 - val_loss: 0.5833 - val_acc: 0.8038
Epoch 18/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5659 - acc: 0.8050 - val_loss: 0.5796 - val_acc: 0.8042
Epoch 19/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5470 - acc: 0.8109 - val_loss: 0.5582 - val_acc: 0.8095
Epoch 20/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5482 - acc: 0.8099 - val_loss: 0.5794 - val_acc: 0.8072
Epoch 21/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5300 - acc: 0.8171 - val_loss: 0.5604 - val_acc: 0.8125
Epoch 22/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5268 - acc: 0.8180 - val_loss: 0.5595 - val_acc: 0.8139
Epoch 23/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5199 - acc: 0.8192 - val_loss: 0.5513 - val_acc: 0.8169
Epoch 24/50
390/390 [==============================] - 11s 29ms/step - loss: 0.5020 - acc: 0.8261 - val_loss: 0.5621 - val_acc: 0.8104
Epoch 25/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4983 - acc: 0.8275 - val_loss: 0.5570 - val_acc: 0.8123
Epoch 26/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4948 - acc: 0.8294 - val_loss: 0.5683 - val_acc: 0.8145
Epoch 27/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4848 - acc: 0.8323 - val_loss: 0.5499 - val_acc: 0.8186
Epoch 28/50
390/390 [==============================] - 12s 30ms/step - loss: 0.4763 - acc: 0.8353 - val_loss: 0.5503 - val_acc: 0.8193
Epoch 29/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4786 - acc: 0.8340 - val_loss: 0.5473 - val_acc: 0.8180
Epoch 30/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4692 - acc: 0.8370 - val_loss: 0.5959 - val_acc: 0.8079
Epoch 31/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4673 - acc: 0.8388 - val_loss: 0.5317 - val_acc: 0.8229
Epoch 32/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4568 - acc: 0.8414 - val_loss: 0.5885 - val_acc: 0.8112
Epoch 33/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4557 - acc: 0.8428 - val_loss: 0.5429 - val_acc: 0.8204
Epoch 34/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4482 - acc: 0.8451 - val_loss: 0.5360 - val_acc: 0.8255
Epoch 35/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4443 - acc: 0.8469 - val_loss: 0.5463 - val_acc: 0.8194
Epoch 36/50
390/390 [==============================] - 12s 30ms/step - loss: 0.4344 - acc: 0.8508 - val_loss: 0.5336 - val_acc: 0.8241
Epoch 37/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4323 - acc: 0.8499 - val_loss: 0.5336 - val_acc: 0.8288
Epoch 38/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4283 - acc: 0.8495 - val_loss: 0.5290 - val_acc: 0.8287
Epoch 39/50
390/390 [==============================] - 11s 28ms/step - loss: 0.4249 - acc: 0.8529 - val_loss: 0.5200 - val_acc: 0.8317
Epoch 40/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4220 - acc: 0.8547 - val_loss: 0.5125 - val_acc: 0.8358
Epoch 41/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4137 - acc: 0.8573 - val_loss: 0.5502 - val_acc: 0.8259
Epoch 42/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4170 - acc: 0.8553 - val_loss: 0.5778 - val_acc: 0.8180
Epoch 43/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4114 - acc: 0.8567 - val_loss: 0.5159 - val_acc: 0.8342
Epoch 44/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4107 - acc: 0.8562 - val_loss: 0.5483 - val_acc: 0.8266
Epoch 45/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4061 - acc: 0.8590 - val_loss: 0.5320 - val_acc: 0.8276
Epoch 46/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4028 - acc: 0.8593 - val_loss: 0.5270 - val_acc: 0.8354
Epoch 47/50
390/390 [==============================] - 11s 29ms/step - loss: 0.4007 - acc: 0.8604 - val_loss: 0.5511 - val_acc: 0.8229
Epoch 48/50
390/390 [==============================] - 11s 29ms/step - loss: 0.3941 - acc: 0.8624 - val_loss: 0.5111 - val_acc: 0.8356
Epoch 49/50
390/390 [==============================] - 11s 29ms/step - loss: 0.3952 - acc: 0.8618 - val_loss: 0.5456 - val_acc: 0.8274
Epoch 50/50
390/390 [==============================] - 11s 29ms/step - loss: 0.3882 - acc: 0.8638 - val_loss: 0.5592 - val_acc: 0.8214
Model took 571.46 seconds to train

Accuracy on test data is: 82.14



